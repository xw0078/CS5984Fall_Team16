{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vector Similarity\n",
    "\n",
    "Use `doc2vec` to find the most similar Wikipedia articles for a given ETD chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/waingram/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk import sent_tokenize, pprint, RegexpTokenizer\n",
    "\n",
    "from experiments.sentence_tokenization import extract_ch_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an ETD that has been processed by Grobid into TEI XML, and extact the chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docPath = '/Users/waingram/Desktop/gorbid_fulltext/theses/17274/Granstedt_JL_T_2017.tei.xml'\n",
    "ch_json = extract_ch_json(docPath, 'thesis', 17292)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved doc2vec model trained on Wikipedia dump. If you don't already have a pretrained model, use `doc2vec_train_wiki_model.py` to create it. NOTE: this model took several days to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('experiments/wikipedia-vectors.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chapter in the ETD, use the model to infer a vector. Then use that vector to find similar wikipedia articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nData Augmentation with Seq2Seq Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/CS5984Fall_Team16/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Corpora in Translation Studies', 0.578441858291626),\n ('Automatic acquisition of sense-tagged corpora', 0.5154329538345337),\n ('Arbitrary-precision arithmetic', 0.5109789967536926),\n ('Moses for Mere Mortals', 0.5108679533004761),\n ('Artificial imagination', 0.5086458921432495),\n ('Dictionary-based machine translation', 0.5022629499435425),\n ('Paraphrasing (computational linguistics)', 0.5017775297164917),\n ('Time delay neural network', 0.5000360012054443),\n ('Statistical machine translation', 0.49955862760543823),\n ('Natural language understanding', 0.49809378385543823)]\n\nIntroduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Paraphrasing (computational linguistics)', 0.5342533588409424),\n ('Statistical machine translation', 0.5028438568115234),\n ('Quantum machine learning', 0.49393802881240845),\n ('Bayesian Program Synthesis', 0.4881432056427002),\n ('Message passing in computer clusters', 0.4806269705295563),\n ('Multi-label classification', 0.47103315591812134),\n ('Domain adaptation', 0.45933797955513),\n ('Conditional random field', 0.45746147632598877),\n ('Europarl Corpus', 0.4558519721031189),\n ('Autoencoder', 0.45458176732063293)]\n\nChapter 2 Literature Review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Paraphrasing (computational linguistics)', 0.5603368282318115),\n ('Statistical machine translation', 0.5517240166664124),\n ('Comparison of different machine translation approaches', 0.5505878329277039),\n ('Automatic acquisition of sense-tagged corpora', 0.5398809313774109),\n ('Automatic summarization', 0.5201674699783325),\n ('Dictionary-based machine translation', 0.5041261315345764),\n ('Message passing in computer clusters', 0.5030425190925598),\n ('Moses (machine translation)', 0.5030009746551514),\n ('Neural machine translation', 0.4965655505657196),\n ('Pivot language', 0.4962734282016754)]\n\nChapter 3 Approach and Discussion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Paraphrasing (computational linguistics)', 0.5562536716461182),\n ('BLEU', 0.5131765604019165),\n ('Plagiarism detection', 0.5048375129699707),\n ('Decompiler', 0.5029244422912598),\n ('Object categorization from image search', 0.5022076368331909),\n ('Weighted Micro Function Points', 0.5009472370147705),\n ('Classic monolingual word-sense disambiguation', 0.4828462302684784),\n ('Automatic acquisition of sense-tagged corpora', 0.48140478134155273),\n ('Latent semantic analysis', 0.47652843594551086),\n ('Variance-based sensitivity analysis', 0.47284334897994995)]\n\nChapter 4 Conclusions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Paraphrasing (computational linguistics)', 0.5084825754165649),\n ('Lazy learning', 0.47415319085121155),\n ('Biasâ€“variance tradeoff', 0.47086840867996216),\n ('PAQ', 0.46527379751205444),\n ('Nati Linial', 0.4645746052265167),\n ('Word2vec', 0.4550856053829193),\n ('BrownBoost', 0.4487757980823517),\n ('Plagiarism detection', 0.44783473014831543),\n ('Reference implementation', 0.44597771763801575),\n ('Kernel perceptron', 0.4430201053619385)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for chapter in ch_json['chapters']:\n",
    "    print()\n",
    "    print(chapter['title'])\n",
    "    sentences = []\n",
    "    for paragraph in chapter['paragraphs']:\n",
    "        sentences += sent_tokenize(paragraph)\n",
    "\n",
    "    doc_words = tokenizer.tokenize(' '.join(sentences).lower())\n",
    "\n",
    "    vector = model.infer_vector(doc_words, steps=200)\n",
    "\n",
    "    sims = model.docvecs.most_similar([vector])\n",
    "    pprint(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
